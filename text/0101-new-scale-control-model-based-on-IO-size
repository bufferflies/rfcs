# New scale control model based on IO size
## Summary
Make every store its own sender 
Motivation
When the difference between the different region's size becomes bigger. It can bring following costs:
- There is a certain relationship between the speed and size of replica migration. By limiting the number of operators to control the replica speed. It cannot adapt to different region sizes, and cannot associated with IO .
- The store limit only controls the storage of the source and target instance. In fact, the speed of the operator is mainly affected by the sender and the receiver, so it may cause a serious imbalance in the sending tasks of the individual nodes.
- The TIKV executes snapshot tasks by FIFO. When the consumption speed of the sender is lower than that of the operator producer, it will take unnecessary wait duration, thereby affecting the timeliness of the operator.  So the task will be held longer and other scheduler cannot scheduler this region. 
Hence, I propose using a new flow mechanism to control the concurrent flow of the sender. It will be helpful to solve the  above problems.
Detailed Design
The key point of the problem is to balance the speed between the  operator producers and  the operator consumers. And the operator consumer can be divided into the snapshot sender (region  leader ) and receiver (new peer). PD can determine the operator's type and quantity according to the task execution situation. 

## Flow Mechanism
In order to prevent PD from over-delivering snapshot tasks to the TIKV instance. Each store will have sliding windows to send snapshots, and the total size of the sliding window can be adjusted by the snapshot execution duration and the operator execution. 
```mermaid
graph LR
        subgraph PD
                        subgraph Store
                                RS[regions]
                                SW[sliding window]
                        end
            SC[Scheduler]-->|1. pick region|RS
            RS -->|2. check store|SW
                        RS -->|3. region available|SC 
            SC -->|4. put operator and take token| OC[operator controller]        
        end 

        
        
        subgraph TIKV-leader
            OC -.->|5. region heartbeat| R[Raft]
            R -.->|6. generate task| P[pool]
            P -->|FIFO|P
            P -.->|7. send task| SE[sender]
                        P -->|7.2 generate duration +|Z
                        SE-->|8.2 send duration +|Z[collect]
            
            Z -.->|9.2 store heartbeat|SW
        end 
        
        subgraph TIKV-learner
                SE -->|8. send snapshot|RE[receiver]
          RE-->|9. apply snapshot|R1[raft]
        end
```

The mechanism of the scheduling limiter is shown in the figure. All scheduler will check the  region that's leader has some available send size. If not , it will try another region as target.

### How to adjust windows size 
The minimum(initial) value of the sliding window is 1000MB. The store can be considered to be scheduled if it's available sending size is bigger than 100MB, so the biggest region can also  be scheduled. 
When the operator execution duration is less than 2 times the snapshot task execution time, the sender windows size will increase , otherwise it will decrease. A PI controller is used to prevent repeated adjustments. It should be noted that when all schedules don't need such big size, it will stop increasing. 

### How to ensure fairness 
The new scheduler policy does not guarantee that the snapshot sending size of all stores is exactly the same, but will switch to the next candidate region when the store has no available sending size. During the prototype testing process on the scale-out and scale-in scene , the sending size on all TIKV reaches the bottleneck. 

### Scheduler 
#### Balance Region Scheduler 
The scheduler picks the store as source first, then picks the region on the source store , finally picks the target store. On the original basis,  it is necessary to check the target region leader whether there is enough sending space to  send. 
#### High Priority Scheduler 
Some scheduler may create a highest priority operator such as balance hot region, region check. For these higher operators, 20% of the send window will be reserved. This can prevent this window size from being completely occupied by low priority task .

## Compatibility
### Multi RocksDB
In this design, the generation of the snapshot will use  check-point hard link to replace the scanning of rocksDB. It can reduce the CPU loads and the duration of snapshot generator. But the snapshots are not compressed ,the network and disk bandwidth will increase.  
The snapshot synchronization process is basically the same, some tasks need to wait to be executed when the store has many snapshot tasks.  The waiting time is still positively related to the backlog of the snapshot task, so feedback mechanism still satisfies this situation. 
## Alternatives
### Follower Replica 
In this design,  a new mechanism in raft algorithm will be introduced  which allows the follower to replaica log or send snapshot to another peer. Since the removed peer can also be the snapshot sender, balance region scheduler no need to consult the region leader whether there is some available sending size. 
But for the other scheduler, there may be a big log gap between the follower and leader, so the follower cannot be the snapshot sender, this may make the operator slower and complicated.
Drawbacks
Due to the need to prevent too many snapshot tasks on the instance, a new limit needs to be added, which may cause the execution priority of the scheduler to change, resulting in a small number of scheduling. 

## Question
1. How about removing the store limit?
The store limiter mainly has the following functions that cannot be replaced:
- Limit the operator generating speed, which can be limited by adjusting the tikv snapshot related parameters later. 
- The second highest (lowest) score store can still be scheduled. This mechanism can effectively promote the fairness of scheduling.
- Remove too many peers from the same store can bring major compaction, which affect the cluster performance.

## Unresolved questions